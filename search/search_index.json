{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Welcome to IVEA Documentation","text":"<p>This is the official documentation for IVEA software.</p> <p>Creating images and videos still in progress.</p>"},{"location":"faq/","title":"Frequently Asked Questions (FAQ)","text":""},{"location":"hotspot-advanced-settings/","title":"Hotspot Area Extraction - Advanced Settings","text":"<p>The Advanced Settings dialog provides additional configuration options for Hotspot Area Extraction. Each parameter is designed to improve event detection accuracy and background noise filtering.</p>"},{"location":"hotspot-advanced-settings/#key-parameters-and-descriptions","title":"Key Parameters and Descriptions","text":""},{"location":"hotspot-advanced-settings/#variation-time-window","title":"Variation Time Window","text":"<ul> <li>Determines the frame subtraction interval:  </li> <li>The algorithm subtracts frame (n+3) from frame (n) to detect variations over time.  </li> </ul>"},{"location":"hotspot-advanced-settings/#exclude-events-in-the-background","title":"Exclude Events in the Background","text":"<ul> <li>Prevents the detection of events occurring in the background.  </li> </ul>"},{"location":"hotspot-advanced-settings/#learn-background-noise-disabled-by-default","title":"Learn Background Noise (Disabled by Default)","text":"<ul> <li>Enables the software to analyze and learn the background noise level.  </li> <li>Note: If an event occurs in the background, this may increase the sensitivity threshold, potentially bypassing weaker events.  </li> </ul>"},{"location":"hotspot-advanced-settings/#frames-to-learn-from","title":"Frames to Learn From","text":"<ul> <li>Defines the number of initial frames used for background noise detection.  </li> <li>Default: first 2 frames (but can be adjusted for more frames as needed).  </li> </ul>"},{"location":"hotspot-advanced-settings/#k-means-clustering-algorithm","title":"K-Means Clustering Algorithm","text":"<ul> <li>Allows users to set the number of clusters (layers) for image segmentation.  </li> <li>If k = 1, the Multilayer Intensity Correction (MIC) functions as a simple ratio equation.  </li> <li>Users can monitor the segmentation results in the results folder and adjust the k value accordingly.  </li> </ul>"},{"location":"hotspot-advanced-settings/#adaptive-threshold-default-0-off","title":"Adaptive Threshold (Default: 0 - Off)","text":"<ul> <li>Enables adaptive thresholding to extract the maximum number of detected regions.  </li> </ul>"},{"location":"hotspot-advanced-settings/#global-threshold-default-0-auto","title":"Global Threshold (Default: 0 - Auto)","text":"<ul> <li>Uses an iterative thresholding method for detecting event regions.  </li> </ul>"},{"location":"hotspot-advanced-settings/#override-global-threshold-not-recommended","title":"Override Global Threshold (Not Recommended)","text":"<ul> <li>Allows users to apply a static threshold instead of the automatic iterative threshold.  </li> </ul>"},{"location":"hotspot-area-extraction/","title":"Hotspot Area Extraction Module","text":"<p>The Hotspot Area Extraction module is designed for analyzing videos with fixed image acquisitions, such as AndromeDA nanosensor paint experiments. This module enables precise event detection and hotspot localization by applying advanced filtering, selection, and brightness correction techniques.  </p>"},{"location":"hotspot-area-extraction/#key-parameters-and-controls","title":"Key Parameters and Controls","text":"<p>Each of the following settings is available as either an input parameter field or a button within the GUI.</p>"},{"location":"hotspot-area-extraction/#noise-filter-radius","title":"Noise Filter Radius","text":"<ul> <li>Uses a median filter to eliminate noise before applying the iterative global threshold.  </li> <li>Default: 8 pixels.  </li> </ul>"},{"location":"hotspot-area-extraction/#selection-enlargement","title":"Selection Enlargement","text":"<ul> <li>Expands the Region of Interest (ROI) by n pixels to include surrounding areas.  </li> </ul>"},{"location":"hotspot-area-extraction/#event-center-search-radius","title":"Event Center Search Radius","text":"<ul> <li>Defines the spatial tracking radius for identifying the center of detected events.  </li> </ul>"},{"location":"hotspot-area-extraction/#brightness-adjustment","title":"Brightness Adjustment","text":"<ul> <li>Controls fluorescence intensity fluctuations using the Multilayer Intensity Correction (MIC) algorithm.  </li> <li>Set this value to 0 if no intensity fluctuations are present in the dataset.  </li> </ul>"},{"location":"hotspot-area-extraction/#create-mask-button","title":"Create Mask Button","text":"<ul> <li>Allows users to manually select a region within the video(s) for noise identification.  </li> <li>Important: Do not select areas where events may occur, as this may affect detection accuracy.  </li> </ul>"},{"location":"hotspot-area-extraction/#advanced-settings-button","title":"Advanced Settings Button","text":"<ul> <li>Opens a dialog window containing default parameters for Hotspot Area Extraction.  </li> </ul>"},{"location":"ivea-plugin/","title":"IVEA Plugin","text":"<p>Author: Eng. Abed Chouaib Supervisors: Dr. Ute Becherer (University of Saarland, CIPMM) | Dr. Ali Shaib (University Medical Center G\u00f6ttingen)  </p>"},{"location":"ivea-plugin/#about","title":"\ud83d\udcd6 About","text":"<p>IVEA (Intelligent Vesicle Exocytosis Analysis Platform) is an open-source Fiji/ImageJ plugin that utilizes artificial intelligence to detect and analyze exocytosis events in a variety of cell types. IVEA is designed to provide fully automated, high-speed batch analysis using state-of-the-art deep learning models.  </p>"},{"location":"ivea-plugin/#ai-powered-exocytosis-detection","title":"\ud83d\udd2c AI-Powered Exocytosis Detection","text":"<p>IVEA integrates advanced Vision Transformer (ViT) and LSTM (Long Short-Term Memory) neural networks, enabling robust detection of random burst exocytosis events and stationary burst events in neurons.  </p> <p>\ud83d\udd39 Publication: Read the Paper </p>"},{"location":"ivea-plugin/#table-of-contents","title":"Table of Contents","text":"<ul> <li>Installation </li> <li>Features </li> </ul>"},{"location":"ivea-plugin/#installation","title":"\ud83d\udee0\ufe0f Installation","text":"<p>1\ufe0f\u20e3 Download the latest IVEA plugin:    - Latest Release    - Version 2 (Paper Results) </p> <p>2\ufe0f\u20e3 Get the source code:    - Source Code </p> <p>3\ufe0f\u20e3 Download Fiji/ImageJ:    - ImageJ Fiji </p> <p>4\ufe0f\u20e3 Download test data:    - Test Data </p> <p>5\ufe0f\u20e3 Install the plugin:    - Option 1: Drag and drop the IVEA .jar file into ImageJ.    - Option 2: Copy and paste the IVEA .jar file into the ImageJ plugin directory, then restart ImageJ.  </p>"},{"location":"ivea-plugin/#features","title":"Features","text":"<p>\u2705 AI-Powered Detection - Employs Vision Transformer (ViT) and LSTM networks to detect exocytosis events across various cell types.  </p> <p>\u2705 Fully Automated Batch Analysis - No manual intervention required\u2014simply load your data, and IVEA will process it automatically.  </p> <p>\u2705 User-Friendly GUI - Intuitive and easy-to-use graphical interface for streamlined analysis.  </p> <p>\u2705 Pretrained Models Included - Multiple pretrained deep learning models available within IVEA.  </p> <p>\u2705 Custom Model Training Support - Users can train and integrate custom models into the IVEA pipeline.  </p> <p>\u2705 Supports Any Image Stack - Compatible with image stacks of any resolution and length.  </p> <p>\u2705 High-Speed Processing - Optimized for efficiency, processing videos in seconds (e.g., 256x256, 3000 frames).  </p>"},{"location":"ivea-plugin/#plugin-location-in-imagej","title":"\ud83d\udcc2 Plugin Location in ImageJ","text":"<p>Once installed, IVEA will appear in ImageJ's plugin menu:  </p> <p>\ud83d\udcc2 Menu Path: <code>Plugins &gt; IVEA</code> </p> <p>IVEA provides multiple analysis modules, each designed for a specific purpose:  </p>"},{"location":"ivea-plugin/#analysis-modules","title":"Analysis Modules","text":"Module Functionality Random Burst Event Detects non-stationary, spontaneous exocytosis events using deep learning models. Stationary Burst Event (Neurons) Identifies exocytosis events localized to specific regions in neurons. Hotspot Area Detection Uses K-means clustering and global iterative thresholding to identify active regions. ROI Multi-Measurement Extracts intensity profiles over a time interval for specified Regions of Interest (ROIs). Data Labeling Provides an annotation tool for training datasets used in IVEA's deep learning models."},{"location":"ivea-plugin/#ivea-resources","title":"\ud83d\udcc2 IVEA Resources","text":"<p>Upon installation, IVEA automatically creates a directory named \"IVEA Resources\" inside the ImageJ Plugins directory.  </p>"},{"location":"ivea-plugin/#contents-of-the-ivea-resources-folder","title":"\ud83d\udcc2 Contents of the <code>IVEA Resources</code> Folder","text":"<ul> <li>Pretrained Deep Learning Models </li> <li>IVEA Software Configuration File </li> </ul> <p>This folder is essential for storing and managing model weights, ensuring the correct functioning of IVEA\u2019s AI-based detection system.  </p>"},{"location":"ivea-plugin/#i-notes","title":"\u2139\ufe0f Notes","text":"<ul> <li>IVEA is continuously evolving, with planned improvements for deep learning models, and advanced data analysis tools.  </li> <li>For updates and community discussions, visit the GitHub Repository.  </li> </ul> <p>We welcome feedback to enhance IVEA\u2019s capabilities! </p>"},{"location":"labeling-new-data/","title":"Labeling New Data","text":"<p>The IVEA Data Labeling Plugin is designed for labeling new data intended for training or refining models. This plugin enables users to efficiently categorize events and artifacts to enhance the performance of IVEA\u2019s machine learning models.</p>"},{"location":"labeling-new-data/#labeling-categories","title":"Labeling Categories","text":""},{"location":"labeling-new-data/#exocytosis-labels-positive-integers","title":"Exocytosis Labels (Positive Integers)","text":"<p>The following labels are assigned to exocytosis events:  </p> <ul> <li><code>0</code> \u2013 Fusion with a cloud  </li> <li><code>1</code> \u2013 Fusion without a cloud  </li> <li><code>2</code> \u2013 Latent vesicle fusion  </li> </ul>"},{"location":"labeling-new-data/#noise-and-artifact-categories-negative-integers","title":"Noise and Artifact Categories (Negative Integers)","text":"<p>Negative integer values are used to classify noise and artifacts:  </p> <ul> <li><code>-1</code> \u2013 Random noise  </li> <li><code>-2</code> \u2013 Vesicle intensity fluctuation  </li> <li><code>-3</code> \u2013 Moving vesicle  </li> <li><code>-4</code> \u2013 Random noise with intensity fluctuations  </li> <li><code>-5</code> \u2013 Intensity flickering and out-of-focus artifact  </li> <li><code>-6</code> \u2013 Intensity rise (vesicle docking)  </li> <li><code>-7</code> \u2013 Intensity decrease (vesicle undocking)  </li> <li><code>-8</code> \u2013 Light movement (e.g., passing light, cloud spreading as a wave, etc.)  </li> </ul>"},{"location":"labeling-new-data/#customizing-labels","title":"Customizing Labels","text":"<p>The available labels are directly controlled by the IVEA JSON configuration file, included with the Python training project.  </p> <p>Users have the ability to:  </p> <ul> <li>Modify existing labels to refine classification criteria.  </li> <li>Add new label categories to expand the dataset's classification capabilities.  </li> <li>Adjust dataset labeling for new datasets or modify existing datasets to fit specific research needs.  </li> </ul> <p>Expert users can further customize and optimize labeling parameters to improve model training and performance.</p>"},{"location":"labeling-rois/","title":"Labeling ROIs with IVEA","text":"<p>IVEA provides an efficient keyboard-driven workflow for labeling Regions of Interest (ROIs) using the ROI Manager. Follow the steps below to quickly and accurately label your data.  </p>"},{"location":"labeling-rois/#navigating-the-roi-manager","title":"\ud83d\udda5\ufe0f Navigating the ROI Manager","text":"<ul> <li>Up (\u2191) / Down (\u2193) Arrow Keys \u2192 Navigate through the list of ROIs.  </li> <li>Enter (\u23ce) \u2192 Open the labeling dialog box for the selected ROI(s).  </li> <li>Shift + Left (\u2190) / Right (\u2192) Arrow Keys \u2192 Move between frames while selecting ROIs.  </li> </ul>"},{"location":"labeling-rois/#assigning-labels","title":"\ud83c\udff7\ufe0f Assigning Labels","text":"<p>1\ufe0f\u20e3 In the labeling dialog box, enter the class number for the selected ROI. 2\ufe0f\u20e3 Press Enter (\u23ce) again to confirm the label. 3\ufe0f\u20e3 Press Esc (\u238b) to close the dialog if needed.  </p>"},{"location":"labeling-rois/#labeling-categories","title":"\ud83c\udff7\ufe0f Labeling Categories","text":"Category Description Example Labels Fusion Events Assign positive integer labels to classify exocytosis fusion events. <code>0</code>, <code>1</code>, <code>2</code>, etc. Noise / Artifacts Assign negative integer labels to classify artifacts and unwanted noise. <code>-1</code>, <code>-2</code>, <code>-3</code>, etc."},{"location":"labeling-rois/#i-optimized-keyboard-workflow","title":"\u2139\ufe0f Optimized Keyboard Workflow","text":"<p>This keyboard shortcut-based approach eliminates excessive mouse interactions, allowing for a fast and streamlined labeling process within IVEA.  </p> <p>By leveraging hotkeys for ROI selection, frame navigation, and label assignment, users can significantly speed up the dataset preparation process for training and analysis.</p>"},{"location":"random-burst-advanced-settings/","title":"Random Burst Event - Advanced Settings","text":"<p>The Advanced Settings dialog provides additional configuration options for Random Burst Event analysis. These settings allow users to fine-tune event detection sensitivity, noise reduction, and fluorescence intensity tracking.</p>"},{"location":"random-burst-advanced-settings/#key-parameters-and-descriptions","title":"Key Parameters and Descriptions","text":""},{"location":"random-burst-advanced-settings/#prominence-default-auto-user-defined-option-p-30","title":"Prominence (Default: Auto, User-Defined Option: p = 30)","text":"<ul> <li>Defines the highest minimum value surrounding a local maximum.</li> <li>This parameter is automatically determined, but users can override it when the signal-to-noise ratio (SNR) is very low.</li> </ul>"},{"location":"random-burst-advanced-settings/#variation-time-window","title":"Variation Time Window","text":"<ul> <li>Defines the frame subtraction interval:</li> <li>The algorithm subtracts frame (n+4) from frame (n) to detect intensity changes over time.</li> </ul>"},{"location":"random-burst-advanced-settings/#event-spread-see-paper-for-details","title":"Event Spread (See Paper for Details)","text":"<ul> <li>Directly influences the Non-Maximum Suppression (NMS) algorithm, which helps filter overlapping detections and improves localization accuracy.</li> </ul>"},{"location":"random-burst-advanced-settings/#neural-network-confidence-default-05","title":"Neural Network Confidence (Default: 0.5)","text":"<ul> <li>Sets the probability threshold for neural network classification.</li> <li>Higher confidence values reduce false positives but may also cause some true events to be missed.</li> </ul>"},{"location":"random-burst-advanced-settings/#nomination-sensitivity-user-defined-option","title":"Nomination Sensitivity (User-Defined Option)","text":"<ul> <li>Adjusts the manual detection threshold sensitivity.</li> <li>Recommended for:</li> <li>Low SNR videos</li> <li>Videos with non-uniform fluorescence intensity (e.g., bright cell regions vs. faded cell regions).</li> </ul>"},{"location":"random-burst-advanced-settings/#gaussian-blur-default-enabled","title":"Gaussian Blur (Default: Enabled)","text":"<ul> <li>Applies a Gaussian blur filter to reduce noise and smooth intensity fluctuations in the dataset.</li> </ul>"},{"location":"random-burst-advanced-settings/#adjust-for-bleach-correction-default-enabled","title":"Adjust for Bleach Correction (Default: Enabled)","text":"<ul> <li>Tracks fluorescence intensity variations over time to correct for photobleaching effects.</li> </ul>"},{"location":"random-burst-advanced-settings/#disable-log-info-prompt-default-disabled","title":"Disable Log Info Prompt (Default: Disabled)","text":"<ul> <li>Prevents IVEA from displaying informational messages in the ImageJ Log window.</li> </ul>"},{"location":"random-burst-event/","title":"Random Burst Event","text":"<p>This module is designed for analyzing mobile granules, including but not limited to T cells, chromaffin cells, INS1 cells, \u03b2 cells, and other exocytotic systems. It facilitates event detection, classification, and measurement using neural network-based analysis techniques.</p>"},{"location":"random-burst-event/#key-parameters-and-features","title":"Key Parameters and Features","text":"<p>Each of the following settings is available as an input field or button within the GUI.</p>"},{"location":"random-burst-event/#vision-radius","title":"Vision Radius","text":"<ul> <li>Default: 14, Minimum: 6 </li> <li>Defines the radius of the neural network visualization area (patch size) surrounding an event.  </li> <li>This parameter is crucial for event classification.  </li> <li>At each ROI center, IVEA extracts the surrounding area over a time interval, enabling the Transformer neural network to analyze this region effectively.  </li> </ul>"},{"location":"random-burst-event/#search-radius","title":"Search Radius","text":"<ul> <li>Determines the ROI radius surrounding an event.  </li> <li>Affects measurement accuracy and the Non-Maximum Suppression (NMS) algorithm.  </li> </ul>"},{"location":"random-burst-event/#temporal-max-pooling","title":"Temporal Max Pooling","text":"<ul> <li>Reduces video frames using the temporal max pooling technique (moving maximum intensity projection).  </li> <li>Useful when:  </li> <li>Event activity exceeds the neural network\u2019s temporal vision limit (26\u201328 frames).  </li> <li>Image acquisition frequency is particularly high (e.g., 50 Hz).  </li> </ul>"},{"location":"random-burst-event/#measurement-window-a-b","title":"Measurement Window (a-b)","text":"<ul> <li>Defines the time interval for event measurement.  </li> <li>The a-b range represents the frame interval used for measurement calculations.  </li> </ul>"},{"location":"random-burst-event/#model-list","title":"Model List","text":"<p>Users can select from the following pretrained models for the Random Burst Events module:  </p> <ul> <li>GranuVision3 (Default Model) </li> <li> <p>Encoder Vision Transformer trained on three exocytosis types:  </p> <ul> <li>Fusion with a cloud  </li> <li>Fusion without a cloud  </li> <li>Latent vesicle fusion  </li> </ul> </li> <li> <p>GranuVision2 </p> </li> <li> <p>Encoder Vision Transformer trained on two exocytosis types:  </p> <ul> <li>Fusion with a cloud  </li> <li>Fusion without a cloud  </li> </ul> </li> <li> <p>GranuLSTM (No longer supported)  </p> </li> <li>While outdated, users can still experiment with it as a general classifier.  </li> </ul>"},{"location":"random-burst-event/#additional-detection-options","title":"Additional Detection Options","text":""},{"location":"random-burst-event/#detect-events-without-cloud-category-label-1","title":"Detect Events Without Cloud (Category Label: 1)","text":"<ul> <li>Enables detection of exocytosis events that do not exhibit an observable cloud.  </li> <li>These events are classified as abrupt disappearances.  </li> </ul>"},{"location":"random-burst-event/#detect-latent-vesicle-fusion-category-label-2","title":"Detect Latent Vesicle Fusion (Category Label: 2)","text":"<ul> <li>Detects exocytosis events that occur suddenly without prior vesicle presence.  </li> </ul>"},{"location":"random-burst-event/#include-smallweak-events-default-enabled","title":"Include Small/Weak Events (Default: Enabled)","text":"<ul> <li>Allows detection of low signal-to-noise ratio (SNR) events.  </li> </ul>"},{"location":"random-burst-event/#additional-features","title":"Additional Features","text":""},{"location":"random-burst-event/#advanced-settings-button","title":"Advanced Settings Button","text":"<ul> <li>Opens a dialog window for configuring default parameters for non-fixed burst event (FBE) analysis.  </li> </ul>"},{"location":"random-burst-event/#custom-model-button","title":"Custom Model Button","text":"<ul> <li>Opens a dialog window that allows users to:  </li> <li>Export training data for model customization.  </li> <li>Integrate custom neural networks into IVEA.  </li> </ul>"},{"location":"random-burst-output-results/","title":"Random Burst Event - Output Results","text":""},{"location":"roi-multi-measurement/","title":"ROI Multi-Measurement Plugin","text":"<p>The IVEA Multi-Measurement ROI Plugin is designed to measure fluorescence intensity profiles over time within selected Regions of Interest (ROIs). This plugin allows users to analyze intensity fluctuations in single-channel or multi-channel videos opened in ImageJ.  </p> <p>It serves as a helper plugin for studying IVEA-detected events or analyzing any ROI manually selected in ImageJ.  </p>"},{"location":"roi-multi-measurement/#key-parameters-and-options","title":"Key Parameters and Options","text":"<p>The plugin provides several customizable options for intensity measurement, available in the user interface.</p>"},{"location":"roi-multi-measurement/#frames-before-user-defined-input-default-20","title":"Frames Before (User-Defined Input, Default: 20)","text":"<ul> <li>Specifies the number of frames before an event to include in the measurement window.  </li> </ul>"},{"location":"roi-multi-measurement/#frames-after-user-defined-input-default-100","title":"Frames After (User-Defined Input, Default: 100)","text":"<ul> <li>Specifies the number of frames after an event to include in the measurement window.  </li> </ul>"},{"location":"roi-multi-measurement/#all-rois-checkbox-default-unchecked","title":"All ROIs (Checkbox, Default: Unchecked)","text":"<ul> <li>If enabled, the plugin measures intensity across all ROIs in the ROI Manager.  </li> </ul>"},{"location":"roi-multi-measurement/#all-opened-images-checkbox-default-unchecked","title":"All Opened Images (Checkbox, Default: Unchecked)","text":"<ul> <li>If enabled, the measurement is performed across all opened video frames.  </li> </ul>"},{"location":"roi-multi-measurement/#save-results-checkbox-default-checked","title":"Save Results (Checkbox, Default: Checked)","text":"<ul> <li>If enabled, the measured intensity values are saved to a specified directory for further analysis.  </li> </ul>"},{"location":"roi-multi-measurement/#f-f0-baseline-subtraction-checkbox-default-unchecked","title":"f - f\u2080 (Baseline Subtraction) (Checkbox, Default: Unchecked)","text":"<ul> <li>If enabled, baseline subtraction is applied using the equation:   [   f - f\u2080   ]   where:  </li> <li>f = Current intensity  </li> <li>f\u2080 = Baseline intensity (pre-event intensity)  </li> </ul>"},{"location":"roi-multi-measurement/#normalize-range-0-1-checkbox-default-unchecked","title":"Normalize Range 0-1 (Checkbox, Default: Unchecked)","text":"<ul> <li>If enabled, intensity values are normalized between 0 and 1, ensuring uniform scaling for comparative analysis.  </li> </ul>"},{"location":"roi-multi-measurement/#save-directory-text-field-browse-button","title":"Save Directory (Text Field &amp; Browse Button)","text":"<ul> <li>Specifies the directory where measurement results will be saved.  </li> <li>Users can manually enter a path or use the Browse button to select a location.  </li> </ul>"},{"location":"roi-multi-measurement/#measure-button-action-button","title":"Measure Button (Action Button)","text":"<ul> <li>Initiates the intensity measurement process based on the selected parameters.  </li> </ul>"},{"location":"stationary-advanced-settings/","title":"Stationary Event Tab - Advanced Settings","text":"<p>The Advanced Settings dialog provides additional configuration options for Stationary Burst Event (SBE) analysis. These parameters allow users to fine-tune event detection, background noise filtering, and neural network sensitivity settings.  </p>"},{"location":"stationary-advanced-settings/#key-parameters-and-descriptions","title":"Key Parameters and Descriptions","text":""},{"location":"stationary-advanced-settings/#erosion-default-0","title":"Erosion (Default: 0)","text":"<ul> <li>Applies an erosion filter to refine detected regions and minimize noise in the image.  </li> </ul>"},{"location":"stationary-advanced-settings/#variation-time-window","title":"Variation Time Window","text":"<ul> <li>Defines the frame subtraction interval:  </li> <li>The algorithm subtracts frame (n+4) from frame (n) to detect changes over time.  </li> </ul>"},{"location":"stationary-advanced-settings/#prominence-default-auto-user-defined-option-p-30","title":"Prominence (Default: Auto, User-Defined Option: p = 30)","text":"<ul> <li>Represents the highest minimum value surrounding a local maximum.  </li> <li>Automatically detected, but users can manually adjust it when the signal-to-noise ratio (SNR) is very low.  </li> </ul>"},{"location":"stationary-advanced-settings/#add-frames-lstm-model-only","title":"Add Frames (LSTM Model Only)","text":"<ul> <li>Extends the analysis window time, increasing the number of frames considered for event detection.  </li> <li>Applicable only to LSTM-based models.  </li> <li>Refer to the associated research paper for detailed methodology.  </li> </ul>"},{"location":"stationary-advanced-settings/#neural-network-confidence-default-05","title":"Neural Network Confidence (Default: 0.5)","text":"<ul> <li>Defines the probability threshold for neural network classification.  </li> <li>Higher confidence values reduce false positives but may also result in missed true events.  </li> </ul>"},{"location":"stationary-advanced-settings/#nomination-sensitivity-user-defined-option","title":"Nomination Sensitivity (User-Defined Option)","text":"<ul> <li>Adjusts the detection threshold sensitivity manually.  </li> <li>Useful when:  </li> <li>The signal-to-noise ratio (SNR) is low.  </li> <li>The video contains non-uniform fluorescence intensity (e.g., bright and dark regions within the same frame).  </li> </ul>"},{"location":"stationary-advanced-settings/#gaussian-blur-default-enabled","title":"Gaussian Blur (Default: Enabled)","text":"<ul> <li>Applies a Gaussian blur filter to reduce noise and smooth intensity variations in the image.  </li> </ul>"},{"location":"stationary-advanced-settings/#disable-log-info-prompt-default-disabled","title":"Disable Log Info Prompt (Default: Disabled)","text":"<ul> <li>Disables IVEA\u2019s informational message prompts in the ImageJ Log window.  </li> </ul>"},{"location":"stationary-event/","title":"Stationary Event Module","text":"<p>The Stationary Event Module is designed for the analysis of videos where stationary events occur at neuronal synapses, such as Dorsal Root Ganglion (DRG) neurons. This module provides precise event detection and processing for stationary burst events in neuronal imaging.  </p>"},{"location":"stationary-event/#key-parameters-and-features","title":"Key Parameters and Features","text":"<p>Each of the following settings is available as an input field or button within the GUI.</p>"},{"location":"stationary-event/#roi-radius","title":"ROI Radius","text":"<ul> <li>Defines the size of the Region of Interest (ROI) surrounding an event.  </li> <li>Directly influences the accuracy of event measurements.  </li> </ul>"},{"location":"stationary-event/#temporal-max-pooling","title":"Temporal Max Pooling","text":"<ul> <li>Applies a temporal max pooling technique (moving maximum intensity projection) to reduce the number of video frames.  </li> <li>Useful in cases where:  </li> <li>Event activity slows down and exceeds the neural network\u2019s temporal vision limit (40 frames).  </li> <li>The image acquisition frequency is high (e.g., 50 Hz).  </li> </ul>"},{"location":"stationary-event/#model-list","title":"Model List","text":"<p>Users can select from the following pretrained models for stationary burst event analysis:  </p> <ul> <li>NeuroLSTM (Default Model)  </li> <li> <p>An LSTM-based model specialized for synaptic neuron exocytosis events.  </p> </li> <li> <p>NeuroVision1 </p> </li> <li> <p>An encoder-based Vision Transformer model trained on three synaptic neuron exocytosis types:  </p> <ul> <li>Short exocytosis  </li> <li>Long-stay exocytosis  </li> <li>Slow exocytosis  </li> </ul> </li> <li> <p>GranuVision3 </p> </li> <li>Another encoder-based Vision Transformer model, trained on granules.  </li> <li>Similar to the model used in the Random Burst Events module.  </li> </ul> <p>Important: - The Vision Transformer models in this module are experimental. - They have been tested on a limited dataset, and performance may vary. - These models are under continuous development and improvement.  </p>"},{"location":"stationary-event/#stimulation-detection","title":"Stimulation Detection","text":""},{"location":"stationary-event/#stimulation-detection-feature","title":"Stimulation Detection Feature","text":"<ul> <li>Designed to detect strong stimulation events, such as those occurring in DRG neurons.  </li> </ul>"},{"location":"stationary-event/#stimulation-a-b","title":"Stimulation (a-b)","text":"<ul> <li>Defines the stimulation timing window:  </li> <li>a-b represents the time range of stimulation.  </li> <li>If a or b is set to <code>0</code>, the values are determined automatically.  </li> <li>Use \"_\" to indicate multiple stimulation events.  </li> </ul>"},{"location":"stationary-event/#additional-features","title":"Additional Features","text":""},{"location":"stationary-event/#advanced-settings-button","title":"Advanced Settings Button","text":"<ul> <li>Opens a dialog window for configuring Fixed Burst Event (FBE) analysis parameters.  </li> </ul>"},{"location":"stationary-event/#measurements-button","title":"Measurements Button","text":"<ul> <li>Opens a dialog window allowing users to customize measurement settings for event analysis.  </li> </ul>"},{"location":"stationary-output-results/","title":"Stationary Event Tab - Output Results","text":""},{"location":"training-configuration-file/","title":"Training Using IVEA Python - Configuration File","text":"<pre><code>## \u25b6\ufe0f IVEA config files parameters adjustment\n\n\n\ud83d\udcc2 **Configuration Files:**  \n- Located in the **`settings`** directory.\n- The main configuration files are:\n  - **`default.json`** \u2013 Used for general training settings.\n  - **`GranuConfig.json`** \u2013 For granular model configurations.\n  - **`NeuroConfig.json`** \u2013 For neural model configurations.\n\n\u2699\ufe0f **How Configuration Selection Works:**  \n- **Model Selection**: Choosing a specific model type will automatically apply the corresponding configuration file.  \n- **New Training Session**: If starting a fresh training session, the system will default to using `default.json`.  \n\nMake sure to adjust the configuration files as needed to match your training requirements.\n</code></pre> <pre><code># \ud83d\udcc4 Configuration File Parameters for IVEA  \n\nThis document explains the parameters within the configuration file for the IVEA software. If any parameter requires additional details, they are marked accordingly.\n\n---\n\n## **Class Mapping (`classes_map`)**  \n\nThe **`classes_map`** section defines the classification labels and their respective properties. Each entry includes:  \n\n- **`class_label`**: A list of numerical labels assigned to this class. A class may have multiple labels, allowing flexibility in data labeling.  \n  - Example: A feature labeled as **`1`** and **`11`** might represent similar but slightly different structures. If the neural network struggles to distinguish them, we can initially assign separate class IDs (e.g., `1` \u2192 `ID 1`, `11` \u2192 `ID 3` for fusion cases). If the network successfully differentiates them, they may share the same ID (`1`).  \n- **`class_id`**: A unique identifier for the class.  \n  - Positive **IDs** correspond to standard classifications.  \n  - Negative **IDs** represent fusion events, ensuring they are correctly processed and stacked accordingly.  \n- **`radius`**: Defines the spatial extent of the class (i.e., the **vesicle/fusion size** as seen by the network).  \n  - If set to `null`, the **default radius** (see `default_radius`) is applied.  \n  - If specified, it overrides the default radius for this class.  \n\n\ud83d\udccc **Adding a New Class**:  \nTo add a new class, simply copy an existing entry within `classes_map`, modify the `class_label`, assign a unique `class_id`, and define an appropriate `radius`.  \n\n```json\n{\"class_label\": [9], \"class_id\": 9, \"radius\": 13}\n</code></pre> <ul> <li>This adds a new class labeled <code>9</code> with ID <code>9</code> and a radius of <code>13</code>.  </li> </ul>"},{"location":"training-configuration-file/#file-dataset-parameters","title":"\ud83d\udcc2 File &amp; Dataset Parameters","text":"Parameter Description <code>class_tag</code> Tag used for class annotation within datasets. <code>roi_file_tag</code> Tag used for labeling Region of Interest (ROI) files used in training. <code>file_extension</code> Defines the format of the input files (default: <code>\"tif\"</code>). <code>outfile_dataset_name</code> Name of the output dataset file. <code>model_name</code> Name of the neural network model used (e.g., <code>\"GranuCalciumVision3\"</code>). <code>model_type</code> Defines the type of model (e.g., <code>\"random\", \"stationary\"</code>). <code>timeseries</code> Number of frames analyzed per input sequence (default: <code>26</code>). <code>more_frames</code> Number of additional frames added after the event for analysis. <code>default_radius</code> The fallback radius used when <code>radius</code> is set to <code>null</code> in <code>classes_map</code>. <code>class_nolabel</code> If a class is not labeled, it is assigned a default label (<code>3</code>). Set this to <code>null</code> to disable automatic labeling of unlabeled regions. <code>radius_nolabel</code> Default radius used for unlabeled classes. <p>\ud83d\udccc Time-Series Breakdown - <code>timeseries = 26</code> \u2192 The vision transformer model processes <code>13</code> frames before and <code>13</code> frames after the event. - <code>more_frames = 2</code> \u2192 Adds two additional frames after the event, increasing the total processed frames to 28 per patch.  </p>"},{"location":"training-configuration-file/#data-handling-parameters","title":"\ud83d\udd27 Data Handling Parameters","text":"Parameter Description <code>num_augmentation</code> Number of augmentation operations applied to the dataset. <code>augment_data</code> Enables (<code>true</code>) or disables (<code>false</code>) data augmentation. <code>save_data</code> Enables (<code>true</code>) or disables (<code>false</code>) saving the processed dataset. <code>output_dir</code> Directory where output data is saved. <code>dataset_path</code> Path to an existing dataset file. <p>\ud83d\udccc Using an Existing Dataset for Training - If training a new model using an existing dataset, provide the <code>.h5</code> dataset file in the <code>dataset_path</code>.   Example: <code>D:/project/CTL-CC-INS_dataset_c11r14_sc16_f3_t26_m2_cd63.h5</code></p>"},{"location":"training-configuration-file/#neural-network-parameters","title":"Neural Network Parameters","text":"Parameter Description <code>epoch_num</code> Number of training epochs. <code>batch_size</code> Number of samples processed per training batch. <code>dropout_rate</code> Dropout rate used to prevent overfitting (<code>0</code> = no dropout). <code>call_patience</code> Number of epochs without improvement before early stopping is triggered. <code>learning_rate</code> The learning rate for the optimizer (<code>3e-4</code> by default). <code>scale_radius_to</code> Rescales all radii to a fixed value (<code>16</code>). Please clarify its exact usage in the network."},{"location":"training-configuration-file/#i-additional-notes","title":"\u2139\ufe0f Additional Notes","text":"<ul> <li>Comments and notes: The config file has comments and notes that have no purpose other than to be informative.</li> </ul> <p>For any custom modifications, ensure that <code>classes_map</code> is correctly structured to reflect your data labeling strategy. ```</p>"},{"location":"training-environment-setup/","title":"Training Using IVEA Python - Environment Setup","text":"<pre><code># IVEA\n\n**Project Name**: IVEA  \n**Python Version**: 3.8.10  \n**TensorFlow Version**: 2.10.1 (with CUDA 11.2 / cuDNN 8.1.1 support)\n\nThis repository (folder named `IVEA_main`) contains all scripts for running the IVEA project. You\u2019ll also find two text files:\n\n1. **`requirements.txt`** \u2013 These are libraries required for the project **other than** TensorFlow.  \n2. **`TensorFlow_Libs.txt`** \u2013 These are TensorFlow + dependencies specifically for TF 2.10.1.\n\n---\n\n## \ud83d\udd27 Recommended Setup\n\nFor simplicity, we **highly recommend** using Anaconda.  \nIf you already have your own Python 3.8.10 environment, you can adapt the steps accordingly.\n\n### \ud83d\udcc2 1. Clone the Repository &amp; Navigate\n\n```bash\ngit clone https://github.com/AbedChouaib/IVEA.git\ncd IVEA/IVEA_main\n</code></pre>"},{"location":"training-environment-setup/#setting-up-the-environment","title":"\ud83d\udee0\ufe0f Setting Up the Environment","text":""},{"location":"training-environment-setup/#2-using-anaconda-recommended","title":"\u2705 2. Using Anaconda (Recommended)","text":"<pre><code>conda create -n IVEA python=3.8.10\nconda activate IVEA\n</code></pre>"},{"location":"training-environment-setup/#3-using-a-virtual-environment-alternative","title":"\u2705 3. Using a Virtual Environment (Alternative)","text":"<p>If you prefer using <code>venv</code> inside the <code>IVEA_main</code> directory instead of Conda:</p> <pre><code>python -m venv venv\n</code></pre> <p>Then, activate the virtual environment:</p> <ul> <li>On Windows:</li> </ul> <p><code>bash   venv\\Scripts\\activate</code></p> <ul> <li>On Mac/Linux:</li> </ul> <p><code>bash   source venv/bin/activate</code></p>"},{"location":"training-environment-setup/#installing-dependencies","title":"\ud83d\udce6 Installing Dependencies","text":""},{"location":"training-environment-setup/#4-install-basic-requirements-before-tensorflow","title":"\ud83d\udee0\ufe0f 4. Install Basic Requirements (Before TensorFlow)","text":"<pre><code>pip install -r requirements.txt\n</code></pre>"},{"location":"training-environment-setup/#5-install-tensorflow-its-dependencies","title":"\ud83d\udee0\ufe0f 5. Install TensorFlow &amp; Its Dependencies","text":"<pre><code>pip install -r TensorFlow_Libs.txt\n</code></pre> <p>Note: We do not use <code>tensorflow-gpu==2.9.1</code> here. Instead, <code>tensorflow==2.10.1</code> will work with GPU support when CUDA 11.2 and cuDNN 8.1.1 are installed.</p>"},{"location":"training-environment-setup/#optional-installing-nvidia-drivers-for-gpu-acceleration","title":"\u26a1 (Optional) Installing NVIDIA Drivers for GPU Acceleration","text":"<p>For GPU support, ensure that:</p> <ol> <li>Your NVIDIA drivers are up to date (supporting CUDA 11.2).</li> <li>You have CUDA 11.2 and cuDNN 8.1.1 installed.</li> </ol> <p>If you\u2019re on Windows, install the CUDA Toolkit and then add cuDNN manually. If you\u2019re on Linux, follow your distro\u2019s instructions or check the NVIDIA documentation.</p> <p>If you don\u2019t need GPU acceleration, you can skip this step.</p>"},{"location":"training-environment-setup/#setting-up-python-interpreter-vs-code","title":"\ud83d\udda5\ufe0f Setting Up Python Interpreter &amp; VS Code","text":"<p>If you\u2019re using VS Code, follow these steps:</p> <ol> <li> <p>Open VS Code and navigate to the <code>IVEA_main</code> folder:    <code>bash    cd path/to/IVEA/IVEA_main    code .</code></p> </li> <li> <p>Set up Python interpreter:</p> </li> <li>Open the Command Palette (<code>Ctrl + Shift + P</code>).</li> <li>Search for \"Python: Select Interpreter\" and click on it.</li> <li> <p>Choose:</p> <ul> <li><code>IVEA</code> (if using Conda)  </li> <li><code>.venv</code> (if using a virtual environment inside <code>IVEA_main</code>)</li> </ul> </li> <li> <p>Ensure VS Code recognizes your environment:</p> </li> <li>If using <code>venv</code>, ensure the <code>Python: Default Interpreter Path</code> is set to:      <code>path/to/IVEA_main/venv/bin/python</code></li> <li>If using Conda, it should be:      <code>path/to/anaconda/envs/IVEA/bin/python</code></li> </ol> <p>Now, you\u2019re ready to run Python scripts inside VS Code!</p> <pre><code>python IVEA_main.py\n</code></pre>"},{"location":"training-environment-setup/#i-final-notes","title":"\u2139\ufe0f Final Notes","text":"<p>\u2705 Make sure you have the correct CUDA/cuDNN versions if you want GPU support. \u2705 Double-check pinned dependencies in <code>TensorFlow_Libs.txt</code> if you run into compatibility issues. \u2705 You can adjust versions in <code>requirements.txt</code>, but ensure compatibility with TensorFlow 2.10.1.</p> <p>Thanks for using IVEA \u2013 this guide should get you up and running quickly! \ud83d\ude80 ```</p>"},{"location":"training-main-script/","title":"Training Using IVEA Python - Main Script GUI","text":"<p>The IVEA_main script serves as the central interface for running the IVEA Python GUI. This section provides detailed instructions on how to train a new model, create datasets, and refine existing models within IVEA.  </p>"},{"location":"training-main-script/#main-script-gui","title":"Main Script GUI","text":""},{"location":"training-main-script/#1-training-a-new-model","title":"1\ufe0f\u20e3 Training a New Model","text":"<p>To train a new model using IVEA, follow these steps:  </p>"},{"location":"training-main-script/#step-1-import-your-dataset","title":"Step 1: Import Your Dataset","text":"<ul> <li>Open the IVEA_main script.  </li> <li>Locate the Your Current Dataset field.  </li> <li>Import the dataset by specifying the path to your HDF5 (.h5) file.  </li> </ul>"},{"location":"training-main-script/#step-2-select-the-training-option","title":"Step 2: Select the Training Option","text":"<ul> <li>In the Select Model dropdown menu, choose Train new model.  </li> </ul>"},{"location":"training-main-script/#step-3-initiate-training","title":"Step 3: Initiate Training","text":"<ul> <li>Click on the Train Model button to start training.  </li> </ul>"},{"location":"training-main-script/#2-creating-a-new-dataset","title":"2\ufe0f\u20e3 Creating a New Dataset","text":"<p>If you do not have an existing dataset, you can generate one from your labeled video files.  </p>"},{"location":"training-main-script/#step-1-specify-the-video-directory","title":"Step 1: Specify the Video Directory","text":"<ul> <li>Locate the Videos Directory (TIFF) field.  </li> <li>Import the directory containing your TIFF video files.  </li> </ul>"},{"location":"training-main-script/#step-2-generate-the-dataset","title":"Step 2: Generate the Dataset","text":"<ul> <li>Click on the Create Dataset button.  </li> <li>The system will automatically process the videos and generate an HDF5 dataset.  </li> <li>Once completed, the new dataset location is loaded automatically.  </li> </ul>"},{"location":"training-main-script/#step-3-train-the-model","title":"Step 3: Train the Model","text":"<ul> <li>Click on the Train Model button to start training using the newly created dataset.  </li> </ul>"},{"location":"training-main-script/#3-combining-datasets","title":"3\ufe0f\u20e3 Combining Datasets","text":"<p>To merge new data with an existing dataset, follow these steps:  </p>"},{"location":"training-main-script/#step-1-import-data-sources","title":"Step 1: Import Data Sources","text":"<ul> <li>Import the directory containing the new video files.  </li> <li>Import the directory of your current dataset.  </li> </ul>"},{"location":"training-main-script/#step-2-combine-the-datasets","title":"Step 2: Combine the Datasets","text":"<ul> <li>Click on the Combine Dataset button.  </li> <li>The datasets will be merged into a larger dataset.  </li> <li>The software will automatically update its reference to the new dataset.  </li> </ul>"},{"location":"training-main-script/#step-3-start-training","title":"Step 3: Start Training","text":"<ul> <li>Click on Train Model to train using the newly combined dataset.  </li> </ul>"},{"location":"training-main-script/#4-refining-an-existing-model","title":"4\ufe0f\u20e3 Refining an Existing Model","text":"<p>IVEA supports model refinement using your custom dataset. Currently, two models are available for refinement:  </p> <ul> <li>GranuVision3 </li> <li>NeuroVision1 </li> </ul>"},{"location":"training-main-script/#step-1-select-the-model-to-refine","title":"Step 1: Select the Model to Refine","text":"<ul> <li>In the Select Model dropdown menu, choose the model you wish to refine.  </li> </ul>"},{"location":"training-main-script/#step-2-provide-training-data","title":"Step 2: Provide Training Data","text":"<ul> <li>Import the directory containing your labeled video files (ROI files, ZIP, or ImageJ ROI files).  </li> </ul>"},{"location":"training-main-script/#step-3-prepare-the-dataset","title":"Step 3: Prepare the Dataset","text":"<ul> <li>Click on the Create Dataset button to process the videos and generate a new dataset.  </li> <li>If you already have a saved dataset, simply specify its directory path.  </li> <li>Once imported, the software remembers your last working directory for future sessions.  </li> </ul>"},{"location":"training-main-script/#step-4-train-the-model","title":"Step 4: Train the Model","text":"<ul> <li>Click on Train Model to refine the selected model.  </li> </ul>"},{"location":"training-main-script/#5-managing-output-files","title":"5\ufe0f\u20e3 Managing Output Files","text":"<p>All output files, including trained models and datasets, are stored in the project directory under IVEA_main.  </p>"},{"location":"training-main-script/#customizing-the-output-directory","title":"Customizing the Output Directory","text":"<ul> <li>Modify the JSON configuration files in the settings directory.  </li> <li>There are three JSON files, each corresponding to a specific model available in the Select Model list.  </li> </ul>"},{"location":"training-running-debugging/","title":"Training Using IVEA Python - Running and Debugging","text":""}]}